{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activation Function.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPvlmmqYY9Gr/Q4kevQSwBd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pin2gupta/Deep-Learning/blob/main/Basics/Activation_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFNZ1ho8jkCI"
      },
      "source": [
        "## What is activation functions?\n",
        "The activation functions are the functions which decides whether the current output of the neuron should be triggered to the next cell or not. It also converts the output to new form that can be accepted to the next layer/neuron.\n",
        "\n",
        "## Why needed?\n",
        "- It is used to add non-linerity into the neural network. \n",
        "- They also help in keeping the value of the output from the neuron restricted to a certain limit as needed. \n",
        "\n",
        "Imagine a neural network without the activation functions. In that case, every neuron will only be performing a linear transformation on the inputs using the weights and biases. Although linear transformations make the neural network simpler, but this network would be less powerful and will not be able to learn the complex patterns from the data\n",
        "\n",
        "\n",
        "## What are types of activation function?\n",
        "####**Heaviside Step Function** \n",
        "This is one of the common activation function in the neural network. \n",
        "\n",
        "\"The function produces 1 (or true) when input passes threshold limit whereas it produces 0 (or false) when input does not pass threshold.\" Since it produces binary results , it is sometimes called binary step function. \n",
        "                \n",
        "                f(x) = 1, if x > 0\n",
        "                f(x) = 0, if x < 0\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1uZDfxRCUHjP4M1DTpoMBkYtBWJW-6-Pj)\n",
        "\n",
        "**Problem with Step Function:** \n",
        "\n",
        "the gradient of the function became zero. This is because there is no component of x in the binary step function.\n",
        "Instead we define Linear Function.\n",
        "\n",
        "###**Linear Activation Function**\n",
        "The function is defined as \n",
        "\n",
        "f(x) = ax\n",
        "here the activation is directly proportional to the input. The variable \"a\" in case could be any constant value. \n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1XpYemvoA31_d_dAFDOtIID8jFlMdKOmW)\n",
        "\n",
        "Here the gradient does not become zero, but it is a constant which does not depend upon the input value of x at all. This implies that the weights and biases will be updated during the backpropagation process but the updating factor would be the same.\n",
        "\n",
        "**Problem with Linear Activation Function**\n",
        "\n",
        "- The neural network will not really improve the error since the gradient is the same for every iteration. \n",
        "- The network will not be able to train well and capture the complex patterns from the data. Hence, linear function might be ideal for simple tasks where interpretability is highly desired.\n",
        "\n",
        "The other activation function as describe in the picture\n",
        "![](https://drive.google.com/uc?export=view&id=1s91Eg7DxA20DGEV03LwLuQqIgTccQnrh)\n",
        "\n",
        "\n",
        "###**Sigmoid :** \n",
        "This is the widely used activation function. THe value ranges from 0 and 1. Unlike the binary step and linear functions, sigmoid is a non-linear function. From the grpah, the gradient values are significant for range -3 and 3 but the graph gets much flatter in the other regions. this implies that the values greater than 3 or less than -3 , will have a very small gradient. As the gradient value approches zero, the network is not really learning. \n",
        "\n",
        "It is generally used for binary classification problems. \n",
        "\n",
        "**Problem with Sigmoid:**\n",
        "- It is computational expensive, causes vanishing gradient problem and not zero-centered. \n",
        "\n",
        "- the sigmoid function is not symmetric around zero. So output of all the neurons will be of the same sign.\n",
        "\n",
        "###**Softmax :** \n",
        "This is generally softmax form of the function. It is used in the multi-class classification problems. \n",
        "\n",
        "###**tanh**\n",
        "This is similar to the sigmoid function. The only difference is that it is symmetric around the origin. It ranges between -1 and 1. This the input won't be always of the same sign.\n",
        "\n",
        "**Problem with tanh:**\n",
        "\n",
        "It resolve only one problem of zero centered but it is technically a sigmoid activation function. \n",
        "\n",
        "###**ReLU**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## How to choose different activation function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9ptOz_lhC5W"
      },
      "source": [
        "### **References:**\n",
        "1. [Everything you need to know about “Activation Functions” in Deep learning models ](https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253)\n",
        "2. [Deep Learning cheatsheet](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning)\n"
      ]
    }
  ]
}