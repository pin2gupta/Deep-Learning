{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.6 Back Propagation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJnP2xB0WsGoVbmXCwGrFU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pin2gupta/Deep-Learning/blob/main/Basics/1_6%20Back_Propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCo6yFTpvPdn"
      },
      "source": [
        "### **Backpropagation** \n",
        "is the heart of every neural network. Firstly, we need to make a distinction between backpropagation and optimizers (which is covered later).\n",
        "\n",
        "Backpropagation is for calculating the gradients efficiently, while optimizers is for training the neural network, using the gradients computed with backpropagation. In short, all backpropagation does for us is compute the gradients.\n",
        "\n",
        "We always start from the output layer and propagate backwards, updating weights and biases for each layer.\n",
        "\n",
        "The idea is simple: adjust the weights and biases throughout the network, so that we get the desired output in the output layer. Say we wanted the output neuron to be 1.0, then we would need to nudge the weights and biases so that we get an output closer to 1.0.\n",
        "\n",
        "We can only change the weights and biases, but activations are direct calculations of those weights and biases, which means we indirectly can adjust every part of the neural network, to get the desired output â€” except for the input layer, since that is the dataset that you input.\n",
        "\n",
        "Optimizers is how the neural networks learn, using backpropagation to calculate the gradients.\n",
        "\n",
        "Many factors contribute to how well a model performs. The way we measure performance, as may be obvious to some, is by a cost function.\n",
        "\n",
        "### **Cost Function**\n",
        "The cost function gives us a value, which we want to optimize. There are too many cost functions to mention them all.\n",
        "\n",
        "### **Chain Rule**\n",
        "Any Joint probability distribution over many random variables may be decomposed into conditional distributions over only variable. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvR_W0i5vRjM"
      },
      "source": [
        "###**References:**\n",
        "\n",
        "1. [Deep Learning: Back Propagation](https://towardsdatascience.com/back-propagation-414ec0043d7#.tje3h7wi0)\n",
        "2. [Back Propagation](https://mlfromscratch.com/neural-networks-explained/#backpropagation)"
      ]
    }
  ]
}