{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.3 Gradient Descent.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNefc6xfvqN6xXmN3Ik/RR1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pin2gupta/Deep-Learning/blob/main/Basics/1_3_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdDU50mU2SOS"
      },
      "source": [
        "##**What is optimization algorithm?**\n",
        "Optimization is the choosing the input to opbtain the best possible output.\n",
        "Algorithm which are used to solve optimization problems are called optimization algorithm. IN Deep Learning optimization is used to optimizr cost function. \n",
        "The type of optimization functions are :\n",
        "- Gradient Descent (GD)\n",
        "- Sofosticated Gardient Descent (SGD)\n",
        "- SGD with minibatch\n",
        "- SGD with Momentum\n",
        "- Netrov Accelerated GRadient(NAG)\n",
        "- Adaptive Gradient(Adagrad)\n",
        "- RootMean Square Propgation (RMSprop)\n",
        "- Adaptive Momentum Estimation(Adam)\n",
        "\n",
        "###**1. Gradient Decent**\n",
        "The gradient descent is the iterative optimization algorithm used in machine learning to minimize a loss function. This function is used to describes how well the model will perform given the current set of parameters ( weight and bias) , and gradient decent is used to find the best set of parameters( coeffecient in linear regression and weights in Deep Learning).\n",
        "\n",
        "Gradient descent is one of the most commonly used optimization algorithms. It is used for minimizing the cost function, which allows us to minimize the error and obtain the lowest possible error value\n",
        "\n",
        "Gradient descent enables a model to learn the gradient or direction that the model should take in order to reduce errors (differences between actual y and predicted y). Direction in the simple linear regression example refers to how the model parameters b0 and b1 should be tweaked or corrected to further reduce the cost function. As the model iterates, it gradually converges towards a minimum where further tweaks to the parameters produce little or zero changes in the loss — also referred to as **convergence**.\n",
        "\n",
        "At this point the model has optimized the weights such that they minimize the cost function. This process is integral (no calculus pun intended!) to the ML process, because it greatly expedites the learning process — you can think of it as a means of receiving corrective feedback on how to improve upon your previous performance.\n",
        "\n",
        "###**How do we calculate the Gradient Descent?**\n",
        "**Step 1:** Initialize the weights(a & b) with random values and calculate Error (SSE)\n",
        "\n",
        "**Step 2:** Calculate the gradient i.e. change in SSE when the weights (a & b) are changed by a very small value from their original randomly initialized value. This helps us move the values of a & b in the direction in which SSE is minimized.\n",
        "\n",
        "**Step 3:** Adjust the weights with the gradients to reach the optimal values where SSE is minimized\n",
        "\n",
        "**Step 4:** Use the new weights for prediction and to calculate the new SSE\n",
        "\n",
        "**Step 5:** Repeat steps 2 and 3 till further adjustments to weights doesn’t significantly reduce the Error\n",
        "\n",
        "**Problem with the Gradient Descent**\n",
        "- Vanishing Gradient \n",
        "- Exploding Gradient\n",
        "- Saddle Point(MiniMax Point)\n",
        "\n",
        "###**2. Sofosticated Gardient Descent (SGD)**\n",
        "\n",
        "\n",
        "###**3.SGD with minibatch**\n",
        "###**4.SGD with Momentum**\n",
        "###**5.Netrov Accelerated GRadient(NAG)**\n",
        "###**6.Adaptive Gradient(Adagrad)**\n",
        "###**7.RootMean Square Propgation (RMSprop)**\n",
        "###**8.Adaptive Momentum Estimation(Adam)**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##When should we use the variants?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfVgIrqghl8d"
      },
      "source": [
        "###**References:**\n",
        "\n",
        "1. [5 Concepts You Should Know About Gradient Descent and Cost Function](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)\n",
        "\n",
        "2. [How to understand Gradient Descent algorithm](https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html)\n",
        "\n",
        "3. [Gradient Descent Problems and Solutions in Neural Networks](https://medium.com/analytics-vidhya/gradient-descent-problems-and-solutions-in-deep-learning-8002bbac09d5)\n",
        "\n",
        "4. [Gradient Descent Algorithm and Its Variants](https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3)"
      ]
    }
  ]
}
